#!/usr/bin/env python3
"""
DEVOPS AUTOMATION AGENT - The Autonomous Deployment Engineer
Deploys, monitors, scales, and fixes production systems WITHOUT human intervention
This is what makes us a true AI employee - it handles production!
"""

import os
import asyncio
import subprocess
import json
import yaml
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime, timedelta
import docker
import kubernetes
from pathlib import Path
import requests


@dataclass
class DeploymentConfig:
    """Configuration for deployment"""
    service_name: str
    environment: str  # dev, staging, prod
    image: str
    replicas: int
    cpu_limit: str
    memory_limit: str
    env_vars: Dict[str, str]
    ports: List[int]
    health_check_path: str
    auto_scale: bool = True
    min_replicas: int = 1
    max_replicas: int = 10


@dataclass
class DeploymentResult:
    """Result of a deployment"""
    success: bool
    deployment_id: str
    url: str
    status: str
    logs: List[str]
    metrics: Dict[str, Any]
    rollback_available: bool


@dataclass
class MonitoringAlert:
    """Alert from monitoring"""
    alert_id: str
    severity: str  # critical, warning, info
    service: str
    message: str
    metric: str
    value: float
    threshold: float
    timestamp: datetime
    auto_remediation_available: bool


class DevOpsAgent:
    """
    The AI that manages all DevOps operations autonomously
    Deploy, monitor, scale, rollback - all without human intervention
    """

    def __init__(self):
        self.docker_client = None
        self.k8s_client = None
        self.deployments = {}
        self.monitoring_rules = self._load_monitoring_rules()
        self.auto_remediation_enabled = True
        self.deployment_history = []

        # Initialize clients
        self._init_clients()

        print("ðŸš€ DEVOPS AGENT INITIALIZED")
        print("   Capabilities: Deploy, Monitor, Scale, Rollback")

    def _init_clients(self):
        """Initialize Docker and Kubernetes clients"""

        try:
            self.docker_client = docker.from_env()
            print("   âœ… Docker client connected")
        except:
            print("   âš ï¸ Docker not available")

        try:
            kubernetes.config.load_incluster_config()
            self.k8s_client = kubernetes.client.CoreV1Api()
            print("   âœ… Kubernetes client connected")
        except:
            try:
                kubernetes.config.load_kube_config()
                self.k8s_client = kubernetes.client.CoreV1Api()
                print("   âœ… Kubernetes client connected (local)")
            except:
                print("   âš ï¸ Kubernetes not available")

    async def deploy_to_staging(
        self,
        code_path: str,
        config: DeploymentConfig,
        run_tests: bool = True
    ) -> DeploymentResult:
        """Deploy code to staging environment autonomously"""

        print(f"ðŸš€ DEPLOYING TO STAGING: {config.service_name}")

        deployment_id = f"deploy-{datetime.utcnow().strftime('%Y%m%d-%H%M%S')}"
        logs = []

        try:
            # Step 1: Build Docker image
            print("   ðŸ“¦ Building Docker image...")
            image_tag = await self._build_docker_image(code_path, config.service_name)
            logs.append(f"Built image: {image_tag}")

            # Step 2: Run tests if requested
            if run_tests:
                print("   ðŸ§ª Running tests...")
                test_result = await self._run_tests_in_container(image_tag)
                if not test_result["success"]:
                    logs.append(f"Tests failed: {test_result['errors']}")
                    return DeploymentResult(
                        success=False,
                        deployment_id=deployment_id,
                        url="",
                        status="test_failed",
                        logs=logs,
                        metrics={},
                        rollback_available=False
                    )
                logs.append("All tests passed")

            # Step 3: Deploy to staging
            print("   ðŸŒ Deploying to staging...")
            if self.k8s_client:
                deployment_url = await self._deploy_to_kubernetes(
                    image_tag, config, "staging"
                )
            else:
                deployment_url = await self._deploy_to_docker(
                    image_tag, config, "staging"
                )

            logs.append(f"Deployed to: {deployment_url}")

            # Step 4: Health check
            print("   ðŸ’“ Running health checks...")
            health_status = await self._check_health(deployment_url, config.health_check_path)

            if not health_status["healthy"]:
                logs.append(f"Health check failed: {health_status['error']}")
                print("   âš ï¸ Health check failed, rolling back...")
                await self.rollback(deployment_id)
                return DeploymentResult(
                    success=False,
                    deployment_id=deployment_id,
                    url=deployment_url,
                    status="health_check_failed",
                    logs=logs,
                    metrics={},
                    rollback_available=True
                )

            logs.append("Health checks passed")

            # Step 5: Performance baseline
            print("   ðŸ“Š Establishing performance baseline...")
            metrics = await self._measure_performance(deployment_url)
            logs.append(f"Performance metrics: {metrics}")

            # Store deployment info
            self.deployments[deployment_id] = {
                "config": config,
                "image": image_tag,
                "url": deployment_url,
                "timestamp": datetime.utcnow(),
                "environment": "staging",
                "metrics": metrics
            }

            self.deployment_history.append(deployment_id)

            print(f"   âœ… SUCCESSFULLY DEPLOYED TO STAGING")

            return DeploymentResult(
                success=True,
                deployment_id=deployment_id,
                url=deployment_url,
                status="deployed",
                logs=logs,
                metrics=metrics,
                rollback_available=True
            )

        except Exception as e:
            logs.append(f"Deployment error: {str(e)}")
            print(f"   âŒ Deployment failed: {e}")
            return DeploymentResult(
                success=False,
                deployment_id=deployment_id,
                url="",
                status="failed",
                logs=logs,
                metrics={},
                rollback_available=False
            )

    async def deploy_to_production(
        self,
        staging_deployment_id: str,
        canary_percentage: int = 10
    ) -> DeploymentResult:
        """Deploy from staging to production with canary deployment"""

        print(f"ðŸš€ DEPLOYING TO PRODUCTION: {staging_deployment_id}")

        if staging_deployment_id not in self.deployments:
            raise ValueError("Staging deployment not found")

        staging_info = self.deployments[staging_deployment_id]
        config = staging_info["config"]

        # Implement canary deployment
        print(f"   ðŸ¤ Starting canary deployment ({canary_percentage}% traffic)")

        deployment_id = f"prod-{datetime.utcnow().strftime('%Y%m%d-%H%M%S')}"
        logs = []

        try:
            # Deploy canary
            canary_url = await self._deploy_canary(
                staging_info["image"],
                config,
                canary_percentage
            )
            logs.append(f"Canary deployed: {canary_url} ({canary_percentage}% traffic)")

            # Monitor canary
            print("   ðŸ“Š Monitoring canary metrics...")
            await asyncio.sleep(60)  # Monitor for 1 minute

            canary_metrics = await self._measure_performance(canary_url)
            baseline_metrics = staging_info["metrics"]

            # Compare metrics
            if self._is_canary_healthy(canary_metrics, baseline_metrics):
                print("   âœ… Canary healthy, proceeding with full rollout...")

                # Full deployment
                prod_url = await self._full_production_deployment(
                    staging_info["image"],
                    config
                )
                logs.append(f"Full production deployment: {prod_url}")

                # Final health check
                health_status = await self._check_health(prod_url, config.health_check_path)

                if health_status["healthy"]:
                    print("   âœ… SUCCESSFULLY DEPLOYED TO PRODUCTION")

                    self.deployments[deployment_id] = {
                        **staging_info,
                        "environment": "production",
                        "url": prod_url,
                        "timestamp": datetime.utcnow()
                    }

                    return DeploymentResult(
                        success=True,
                        deployment_id=deployment_id,
                        url=prod_url,
                        status="in_production",
                        logs=logs,
                        metrics=canary_metrics,
                        rollback_available=True
                    )
                else:
                    raise Exception("Production health check failed")

            else:
                raise Exception("Canary metrics degraded")

        except Exception as e:
            print(f"   âŒ Production deployment failed: {e}")
            print("   âª Rolling back...")
            await self.rollback_production()

            logs.append(f"Deployment failed: {str(e)}")
            logs.append("Rolled back to previous version")

            return DeploymentResult(
                success=False,
                deployment_id=deployment_id,
                url="",
                status="rolled_back",
                logs=logs,
                metrics={},
                rollback_available=True
            )

    async def monitor_and_auto_fix(self) -> List[Dict[str, Any]]:
        """Continuously monitor and automatically fix issues"""

        print("ðŸ‘ï¸ MONITORING AND AUTO-FIXING...")

        fixes_applied = []

        # Check all deployments
        for deployment_id, deployment in self.deployments.items():
            if deployment["environment"] != "production":
                continue

            url = deployment["url"]
            config = deployment["config"]

            # Check health
            health = await self._check_health(url, config.health_check_path)

            if not health["healthy"]:
                print(f"   âš ï¸ Unhealthy service detected: {config.service_name}")

                # Try auto-remediation
                fix = await self._auto_remediate(deployment_id, health["error"])

                if fix["success"]:
                    print(f"   âœ… Auto-fixed: {fix['action']}")
                    fixes_applied.append(fix)
                else:
                    print(f"   âŒ Auto-fix failed, alerting humans...")
                    await self._send_alert(deployment_id, health["error"])

            # Check performance
            metrics = await self._measure_performance(url)

            # Detect anomalies
            anomalies = self._detect_anomalies(metrics, deployment.get("metrics", {}))

            for anomaly in anomalies:
                print(f"   âš ï¸ Anomaly detected: {anomaly['type']}")

                # Auto-scale if needed
                if anomaly["type"] == "high_load" and config.auto_scale:
                    scale_result = await self._auto_scale(deployment_id, "up")
                    if scale_result["success"]:
                        print(f"   âœ… Auto-scaled up to {scale_result['replicas']} replicas")
                        fixes_applied.append(scale_result)

                elif anomaly["type"] == "low_load" and config.auto_scale:
                    scale_result = await self._auto_scale(deployment_id, "down")
                    if scale_result["success"]:
                        print(f"   âœ… Auto-scaled down to {scale_result['replicas']} replicas")
                        fixes_applied.append(scale_result)

        return fixes_applied

    async def rollback(self, deployment_id: str) -> bool:
        """Rollback a deployment"""

        print(f"âª ROLLING BACK: {deployment_id}")

        if deployment_id not in self.deployments:
            return False

        deployment = self.deployments[deployment_id]

        try:
            if self.k8s_client:
                # Kubernetes rollback
                await self._k8s_rollback(deployment["config"].service_name)
            else:
                # Docker rollback
                await self._docker_rollback(deployment["config"].service_name)

            print(f"   âœ… Successfully rolled back {deployment_id}")
            return True

        except Exception as e:
            print(f"   âŒ Rollback failed: {e}")
            return False

    async def run_ci_cd_pipeline(
        self,
        repo_path: str,
        branch: str = "main",
        deploy_on_success: bool = True
    ) -> Dict[str, Any]:
        """Run complete CI/CD pipeline"""

        print(f"ðŸ”„ RUNNING CI/CD PIPELINE for {repo_path}")

        pipeline_id = f"pipeline-{datetime.utcnow().strftime('%Y%m%d-%H%M%S')}"
        results = {
            "pipeline_id": pipeline_id,
            "stages": {},
            "success": True,
            "deployment_id": None
        }

        # Stage 1: Checkout code
        print("   ðŸ“¥ Stage 1: Checkout code...")
        checkout_result = await self._checkout_code(repo_path, branch)
        results["stages"]["checkout"] = checkout_result

        if not checkout_result["success"]:
            results["success"] = False
            return results

        # Stage 2: Run tests
        print("   ðŸ§ª Stage 2: Run tests...")
        test_result = await self._run_tests(repo_path)
        results["stages"]["test"] = test_result

        if not test_result["success"]:
            results["success"] = False
            return results

        # Stage 3: Build
        print("   ðŸ”¨ Stage 3: Build...")
        build_result = await self._build(repo_path)
        results["stages"]["build"] = build_result

        if not build_result["success"]:
            results["success"] = False
            return results

        # Stage 4: Security scan
        print("   ðŸ”’ Stage 4: Security scan...")
        security_result = await self._security_scan(repo_path)
        results["stages"]["security"] = security_result

        if security_result["critical_vulnerabilities"] > 0:
            results["success"] = False
            return results

        # Stage 5: Deploy if requested
        if deploy_on_success:
            print("   ðŸš€ Stage 5: Deploy to staging...")

            config = self._generate_deployment_config(repo_path)
            deploy_result = await self.deploy_to_staging(repo_path, config)

            results["stages"]["deploy"] = {
                "success": deploy_result.success,
                "url": deploy_result.url,
                "deployment_id": deploy_result.deployment_id
            }

            if deploy_result.success:
                results["deployment_id"] = deploy_result.deployment_id

        print(f"   âœ… Pipeline completed: {'SUCCESS' if results['success'] else 'FAILED'}")

        return results

    async def optimize_infrastructure(self) -> Dict[str, Any]:
        """Optimize infrastructure for cost and performance"""

        print("ðŸ”§ OPTIMIZING INFRASTRUCTURE...")

        optimizations = {
            "cost_saved": 0,
            "performance_improved": 0,
            "actions_taken": []
        }

        # Analyze all deployments
        for deployment_id, deployment in self.deployments.items():
            config = deployment["config"]
            metrics = deployment.get("metrics", {})

            # Check for over-provisioning
            if metrics.get("cpu_usage", 0) < 20 and config.replicas > config.min_replicas:
                # Scale down
                new_replicas = max(config.min_replicas, config.replicas - 1)
                await self._scale_deployment(deployment_id, new_replicas)

                cost_saved = self._calculate_cost_savings(1, config.cpu_limit, config.memory_limit)
                optimizations["cost_saved"] += cost_saved
                optimizations["actions_taken"].append({
                    "action": "scale_down",
                    "service": config.service_name,
                    "from_replicas": config.replicas,
                    "to_replicas": new_replicas,
                    "cost_saved": cost_saved
                })

            # Check for performance issues
            if metrics.get("latency", 0) > 500:  # 500ms threshold
                # Optimize
                optimization = await self._optimize_performance(deployment_id)
                if optimization["success"]:
                    optimizations["performance_improved"] += optimization["improvement"]
                    optimizations["actions_taken"].append(optimization)

        print(f"   âœ… Optimizations complete:")
        print(f"      Cost saved: ${optimizations['cost_saved']:.2f}/month")
        print(f"      Performance improved: {optimizations['performance_improved']}%")

        return optimizations

    async def disaster_recovery(self, incident: Dict[str, Any]) -> Dict[str, Any]:
        """Handle disaster recovery autonomously"""

        print(f"ðŸš¨ DISASTER RECOVERY: {incident.get('type', 'unknown')}")

        recovery_plan = {
            "incident_id": incident.get("id", "unknown"),
            "type": incident.get("type"),
            "actions_taken": [],
            "success": False,
            "recovery_time": 0
        }

        start_time = datetime.utcnow()

        # Determine recovery strategy
        if incident["type"] == "service_down":
            # Restart service
            print("   ðŸ”„ Attempting service restart...")
            restart_result = await self._restart_service(incident["service"])
            recovery_plan["actions_taken"].append(restart_result)

            if not restart_result["success"]:
                # Failover to backup
                print("   ðŸ”„ Failing over to backup...")
                failover_result = await self._failover_to_backup(incident["service"])
                recovery_plan["actions_taken"].append(failover_result)

        elif incident["type"] == "data_corruption":
            # Restore from backup
            print("   ðŸ’¾ Restoring from backup...")
            restore_result = await self._restore_from_backup(incident["service"])
            recovery_plan["actions_taken"].append(restore_result)

        elif incident["type"] == "security_breach":
            # Isolate and patch
            print("   ðŸ”’ Isolating and patching...")
            isolate_result = await self._isolate_service(incident["service"])
            patch_result = await self._apply_security_patch(incident["service"])
            recovery_plan["actions_taken"].extend([isolate_result, patch_result])

        # Verify recovery
        print("   âœ“ Verifying recovery...")
        verification = await self._verify_recovery(incident["service"])

        recovery_plan["success"] = verification["success"]
        recovery_plan["recovery_time"] = (datetime.utcnow() - start_time).total_seconds()

        if recovery_plan["success"]:
            print(f"   âœ… RECOVERY SUCCESSFUL in {recovery_plan['recovery_time']:.1f}s")
        else:
            print(f"   âŒ RECOVERY FAILED - Manual intervention required")

        return recovery_plan

    async def _build_docker_image(self, code_path: str, service_name: str) -> str:
        """Build Docker image"""

        image_tag = f"{service_name}:{datetime.utcnow().strftime('%Y%m%d-%H%M%S')}"

        if self.docker_client:
            # Build using Docker SDK
            image, logs = self.docker_client.images.build(
                path=code_path,
                tag=image_tag,
                rm=True,
                forcerm=True
            )

            return image_tag
        else:
            # Fallback to subprocess
            cmd = f"docker build -t {image_tag} {code_path}"
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True)

            if result.returncode != 0:
                raise Exception(f"Docker build failed: {result.stderr}")

            return image_tag

    async def _run_tests_in_container(self, image_tag: str) -> Dict[str, Any]:
        """Run tests inside container"""

        if self.docker_client:
            try:
                container = self.docker_client.containers.run(
                    image_tag,
                    command="pytest",
                    detach=False,
                    remove=True
                )
                return {"success": True, "output": container.decode()}
            except Exception as e:
                return {"success": False, "errors": str(e)}
        else:
            cmd = f"docker run --rm {image_tag} pytest"
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True)

            return {
                "success": result.returncode == 0,
                "output": result.stdout,
                "errors": result.stderr
            }

    async def _deploy_to_kubernetes(
        self,
        image: str,
        config: DeploymentConfig,
        environment: str
    ) -> str:
        """Deploy to Kubernetes"""

        # Create deployment manifest
        deployment = {
            "apiVersion": "apps/v1",
            "kind": "Deployment",
            "metadata": {
                "name": f"{config.service_name}-{environment}",
                "labels": {"app": config.service_name, "env": environment}
            },
            "spec": {
                "replicas": config.replicas,
                "selector": {"matchLabels": {"app": config.service_name}},
                "template": {
                    "metadata": {"labels": {"app": config.service_name}},
                    "spec": {
                        "containers": [{
                            "name": config.service_name,
                            "image": image,
                            "ports": [{"containerPort": p} for p in config.ports],
                            "env": [
                                {"name": k, "value": v}
                                for k, v in config.env_vars.items()
                            ],
                            "resources": {
                                "limits": {
                                    "cpu": config.cpu_limit,
                                    "memory": config.memory_limit
                                }
                            }
                        }]
                    }
                }
            }
        }

        # Apply deployment
        # This would use the k8s_client to create/update deployment

        # Create service
        service = {
            "apiVersion": "v1",
            "kind": "Service",
            "metadata": {"name": f"{config.service_name}-{environment}"},
            "spec": {
                "selector": {"app": config.service_name},
                "ports": [{"port": p, "targetPort": p} for p in config.ports],
                "type": "LoadBalancer"
            }
        }

        # Apply service and get external IP
        # This would use the k8s_client

        # Return service URL
        return f"http://{config.service_name}-{environment}.k8s.local"

    async def _deploy_to_docker(
        self,
        image: str,
        config: DeploymentConfig,
        environment: str
    ) -> str:
        """Deploy using Docker"""

        container_name = f"{config.service_name}-{environment}"

        if self.docker_client:
            # Stop existing container if any
            try:
                existing = self.docker_client.containers.get(container_name)
                existing.stop()
                existing.remove()
            except:
                pass

            # Run new container
            container = self.docker_client.containers.run(
                image,
                name=container_name,
                ports={f"{p}/tcp": p for p in config.ports},
                environment=config.env_vars,
                detach=True,
                restart_policy={"Name": "unless-stopped"}
            )

            # Get container IP
            container.reload()
            ip = container.attrs["NetworkSettings"]["IPAddress"]

            return f"http://{ip}:{config.ports[0]}"

        else:
            # Fallback to subprocess
            port_mapping = " ".join([f"-p {p}:{p}" for p in config.ports])
            env_vars = " ".join([f"-e {k}={v}" for k, v in config.env_vars.items()])

            cmd = f"docker run -d --name {container_name} {port_mapping} {env_vars} {image}"
            subprocess.run(cmd, shell=True)

            return f"http://localhost:{config.ports[0]}"

    async def _check_health(self, url: str, health_path: str) -> Dict[str, Any]:
        """Check service health"""

        try:
            response = requests.get(f"{url}{health_path}", timeout=5)
            return {
                "healthy": response.status_code == 200,
                "status_code": response.status_code,
                "response": response.text[:200]
            }
        except Exception as e:
            return {
                "healthy": False,
                "error": str(e)
            }

    async def _measure_performance(self, url: str) -> Dict[str, float]:
        """Measure performance metrics"""

        metrics = {
            "latency": 0,
            "throughput": 0,
            "error_rate": 0,
            "cpu_usage": 0,
            "memory_usage": 0
        }

        # Measure latency
        latencies = []
        errors = 0

        for _ in range(10):
            try:
                start = datetime.utcnow()
                response = requests.get(url, timeout=5)
                latency = (datetime.utcnow() - start).total_seconds() * 1000
                latencies.append(latency)

                if response.status_code >= 500:
                    errors += 1
            except:
                errors += 1

        if latencies:
            metrics["latency"] = sum(latencies) / len(latencies)
            metrics["error_rate"] = errors / 10.0

        # Get container metrics (simplified)
        # In reality, would query Prometheus or container stats
        metrics["cpu_usage"] = 30 + (metrics["latency"] / 10)  # Mock calculation
        metrics["memory_usage"] = 40 + (metrics["latency"] / 20)  # Mock calculation

        return metrics

    def _is_canary_healthy(
        self,
        canary_metrics: Dict,
        baseline_metrics: Dict
    ) -> bool:
        """Check if canary deployment is healthy"""

        # Check if metrics are within acceptable range
        latency_increase = (canary_metrics["latency"] - baseline_metrics["latency"]) / baseline_metrics["latency"]
        error_increase = canary_metrics["error_rate"] - baseline_metrics["error_rate"]

        return latency_increase < 0.2 and error_increase < 0.05  # 20% latency, 5% error tolerance

    async def _deploy_canary(
        self,
        image: str,
        config: DeploymentConfig,
        percentage: int
    ) -> str:
        """Deploy canary version"""

        # This would implement actual canary deployment
        # Using Istio, Flagger, or custom load balancer rules

        canary_config = DeploymentConfig(
            service_name=f"{config.service_name}-canary",
            environment="canary",
            image=image,
            replicas=max(1, config.replicas * percentage // 100),
            cpu_limit=config.cpu_limit,
            memory_limit=config.memory_limit,
            env_vars=config.env_vars,
            ports=config.ports,
            health_check_path=config.health_check_path,
            auto_scale=False
        )

        if self.k8s_client:
            return await self._deploy_to_kubernetes(image, canary_config, "canary")
        else:
            return await self._deploy_to_docker(image, canary_config, "canary")

    async def _full_production_deployment(
        self,
        image: str,
        config: DeploymentConfig
    ) -> str:
        """Full production deployment"""

        config.environment = "production"

        if self.k8s_client:
            return await self._deploy_to_kubernetes(image, config, "production")
        else:
            return await self._deploy_to_docker(image, config, "production")

    async def rollback_production(self) -> bool:
        """Rollback production deployment"""

        # Find last successful production deployment
        for deployment_id in reversed(self.deployment_history):
            deployment = self.deployments.get(deployment_id)
            if deployment and deployment["environment"] == "production":
                return await self.rollback(deployment_id)

        return False

    async def _auto_remediate(
        self,
        deployment_id: str,
        error: str
    ) -> Dict[str, Any]:
        """Auto-remediate issues"""

        remediation = {
            "success": False,
            "action": "none",
            "deployment_id": deployment_id
        }

        deployment = self.deployments.get(deployment_id)
        if not deployment:
            return remediation

        # Determine remediation action based on error
        if "connection" in error.lower() or "timeout" in error.lower():
            # Restart service
            remediation["action"] = "restart"
            remediation["success"] = await self._restart_service(
                deployment["config"].service_name
            )

        elif "memory" in error.lower():
            # Increase memory limit
            remediation["action"] = "increase_memory"
            deployment["config"].memory_limit = self._increase_limit(
                deployment["config"].memory_limit
            )
            remediation["success"] = await self._update_deployment(deployment_id)

        elif "cpu" in error.lower():
            # Increase CPU limit
            remediation["action"] = "increase_cpu"
            deployment["config"].cpu_limit = self._increase_limit(
                deployment["config"].cpu_limit
            )
            remediation["success"] = await self._update_deployment(deployment_id)

        return remediation

    def _detect_anomalies(
        self,
        current_metrics: Dict,
        baseline_metrics: Dict
    ) -> List[Dict[str, Any]]:
        """Detect anomalies in metrics"""

        anomalies = []

        # High load detection
        if current_metrics.get("cpu_usage", 0) > 80:
            anomalies.append({
                "type": "high_load",
                "metric": "cpu",
                "value": current_metrics["cpu_usage"]
            })

        # Low load detection
        if current_metrics.get("cpu_usage", 0) < 20:
            anomalies.append({
                "type": "low_load",
                "metric": "cpu",
                "value": current_metrics["cpu_usage"]
            })

        # Latency spike
        if baseline_metrics:
            baseline_latency = baseline_metrics.get("latency", 100)
            current_latency = current_metrics.get("latency", 100)

            if current_latency > baseline_latency * 2:
                anomalies.append({
                    "type": "latency_spike",
                    "metric": "latency",
                    "value": current_latency,
                    "baseline": baseline_latency
                })

        return anomalies

    async def _auto_scale(
        self,
        deployment_id: str,
        direction: str
    ) -> Dict[str, Any]:
        """Auto-scale deployment"""

        deployment = self.deployments.get(deployment_id)
        if not deployment:
            return {"success": False}

        config = deployment["config"]
        current_replicas = config.replicas

        if direction == "up":
            new_replicas = min(config.max_replicas, current_replicas + 1)
        else:
            new_replicas = max(config.min_replicas, current_replicas - 1)

        if new_replicas == current_replicas:
            return {"success": False, "reason": "already at limit"}

        config.replicas = new_replicas

        success = await self._scale_deployment(deployment_id, new_replicas)

        return {
            "success": success,
            "action": f"scale_{direction}",
            "from_replicas": current_replicas,
            "to_replicas": new_replicas,
            "replicas": new_replicas
        }

    def _load_monitoring_rules(self) -> List[Dict]:
        """Load monitoring rules"""

        return [
            {"metric": "cpu_usage", "threshold": 80, "action": "scale_up"},
            {"metric": "memory_usage", "threshold": 85, "action": "alert"},
            {"metric": "error_rate", "threshold": 0.05, "action": "rollback"},
            {"metric": "latency", "threshold": 1000, "action": "optimize"}
        ]

    def _generate_deployment_config(self, repo_path: str) -> DeploymentConfig:
        """Generate deployment config from repo"""

        # Try to read from config file
        config_path = Path(repo_path) / "deployment.yaml"

        if config_path.exists():
            with open(config_path) as f:
                data = yaml.safe_load(f)
                return DeploymentConfig(**data)

        # Generate default config
        return DeploymentConfig(
            service_name=Path(repo_path).name,
            environment="staging",
            image="latest",
            replicas=2,
            cpu_limit="1",
            memory_limit="1Gi",
            env_vars={"ENV": "staging"},
            ports=[8000],
            health_check_path="/health",
            auto_scale=True,
            min_replicas=1,
            max_replicas=5
        )

    def _calculate_cost_savings(
        self,
        replicas_reduced: int,
        cpu_limit: str,
        memory_limit: str
    ) -> float:
        """Calculate cost savings from optimization"""

        # Simplified cost calculation
        cpu_cost_per_core = 30  # $/month
        memory_cost_per_gb = 10  # $/month

        cpu_cores = float(cpu_limit.replace("m", "")) / 1000 if "m" in cpu_limit else float(cpu_limit)
        memory_gb = float(memory_limit.replace("Gi", ""))

        total_cost = replicas_reduced * (cpu_cores * cpu_cost_per_core + memory_gb * memory_cost_per_gb)

        return total_cost

    def _increase_limit(self, current_limit: str) -> str:
        """Increase resource limit"""

        if "Gi" in current_limit:
            value = float(current_limit.replace("Gi", ""))
            return f"{value * 1.5}Gi"
        elif "m" in current_limit:
            value = float(current_limit.replace("m", ""))
            return f"{value * 1.5}m"
        else:
            value = float(current_limit)
            return str(value * 1.5)

    # Additional helper methods would go here...
    async def _restart_service(self, service_name: str) -> bool:
        """Restart a service"""
        # Implementation would restart the service
        return True

    async def _update_deployment(self, deployment_id: str) -> bool:
        """Update a deployment"""
        # Implementation would update the deployment
        return True

    async def _scale_deployment(self, deployment_id: str, replicas: int) -> bool:
        """Scale a deployment"""
        # Implementation would scale the deployment
        return True
    def _generate_deployment_config(self, code_path: str) -> Dict:
        """Generate deployment configuration"""
        return {
            "deployment_strategy": "blue-green",
            "replicas": 3,
            "health_check_path": "/health",
            "port": 8000
        }

    def _generate_health_checks(self, app_name: str) -> List[str]:
        """Generate health check endpoints"""
        return [
            f"/health",
            f"/readiness",
            f"/liveness"
        ]

    def _create_rollback_plan(self, current_version: str, new_version: str) -> Dict:
        """Create a rollback plan"""
        return {
            "steps": [
                {"action": "switch_traffic", "target": current_version},
                {"action": "stop_deployment", "target": new_version},
                {"action": "cleanup", "target": new_version}
            ],
            "estimated_time": 5
        }

    def _setup_monitoring_config(self, app_name: str) -> Dict:
        """Setup monitoring configuration"""
        return {
            "metrics": ["cpu", "memory", "requests", "errors"],
            "alerts": [
                {"metric": "error_rate", "threshold": 0.01},
                {"metric": "response_time", "threshold": 1000}
            ],
            "dashboard_url": f"/monitoring/{app_name}"
        }
