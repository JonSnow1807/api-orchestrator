"""
LLM Decision Engine - Ultra Premium Feature
Enables autonomous AI agents that make intelligent decisions and execute actions
"""

import json
import asyncio
import os
from typing import Dict, List, Any, Optional
from datetime import datetime
from dataclasses import dataclass
from enum import Enum
import logging

# Conditional imports for production resilience
try:
    import anthropic

    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False
    print("âš ï¸ Warning: anthropic package not available, LLM decisions disabled")

try:
    import openai

    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("âš ï¸ Warning: openai package not available, falling back to anthropic only")


class DecisionType(Enum):
    """Types of decisions the LLM can make"""

    ANALYSIS_PLAN = "analysis_plan"
    ACTION_SEQUENCE = "action_sequence"
    TOOL_SELECTION = "tool_selection"
    RISK_ASSESSMENT = "risk_assessment"
    OPTIMIZATION_STRATEGY = "optimization_strategy"


class RiskLevel(Enum):
    """Risk levels for automated actions"""

    SAFE = "safe"  # No risk, auto-execute
    LOW = "low"  # Minor risk, auto-execute with logging
    MEDIUM = "medium"  # Some risk, require user approval
    HIGH = "high"  # High risk, require explicit user confirmation
    CRITICAL = "critical"  # Never auto-execute, manual only


@dataclass
class DecisionContext:
    """Context information for LLM decision making"""

    user_id: str
    project_id: str
    endpoint_data: Dict[str, Any]
    historical_data: List[Dict[str, Any]]
    user_preferences: Dict[str, Any]
    available_tools: List[str]
    current_findings: Dict[str, Any]
    business_context: Optional[str] = None


@dataclass
class AgentAction:
    """Represents an action an agent can take"""

    action_id: str
    tool_name: str
    parameters: Dict[str, Any]
    risk_level: RiskLevel
    reasoning: str
    expected_outcome: str
    estimated_duration: int  # seconds
    depends_on: Optional[List[str]] = None  # Other action IDs this depends on


@dataclass
class DecisionPlan:
    """A complete plan of actions decided by the LLM"""

    plan_id: str
    decision_type: DecisionType
    actions: List[AgentAction]
    total_estimated_duration: int
    confidence_score: float
    reasoning: str
    risk_assessment: RiskLevel
    requires_approval: bool
    created_at: datetime


class LLMDecisionEngine:
    """
    Core LLM Decision Engine for Ultra Premium autonomous agents
    """

    def __init__(self, api_key: str = None, provider: str = "anthropic"):
        self.provider = provider
        self.anthropic_client = None
        self.openai_client = None
        self.logger = logging.getLogger(__name__)

        # Initialize the appropriate client
        self._initialize_clients(api_key)

        # Decision history for learning
        self.decision_history: List[DecisionPlan] = []

        # Tool registry - maps tool names to descriptions
        self.tool_registry = {
            "security_vulnerability_scan": "Scans for security vulnerabilities using OWASP standards",
            "auth_mechanism_analysis": "Analyzes authentication and authorization mechanisms",
            "data_exposure_check": "Checks for potential sensitive data exposure",
            "rate_limiting_test": "Tests for rate limiting and DoS protection",
            "ssl_configuration_check": "Verifies SSL/TLS configuration and certificates",
            "input_validation_test": "Tests input validation and sanitization",
            "session_management_audit": "Audits session management security",
            "cors_policy_analysis": "Analyzes CORS policy configuration",
            "header_security_scan": "Scans security headers and configurations",
            "compliance_check": "Checks compliance with standards (GDPR, HIPAA, PCI-DSS)",
            "auto_fix_security_headers": "Automatically fixes common security header issues",
            "auto_update_environment_vars": "Updates environment variables with secure defaults",
            "auto_generate_tests": "Generates comprehensive test suites",
            "auto_optimize_performance": "Optimizes API performance automatically",
        }

    def _initialize_clients(self, api_key: str = None):
        """Initialize LLM clients with production-grade features"""
        # Production LLM configuration
        anthropic_key = api_key or os.getenv("ANTHROPIC_API_KEY")
        openai_key = api_key or os.getenv("OPENAI_API_KEY")

        # Rate limiting and cost tracking
        self.request_count = 0
        self.daily_cost = 0.0
        self.max_daily_cost = float(os.getenv("LLM_MAX_DAILY_COST", "100.0"))
        self.rate_limit_per_minute = int(os.getenv("LLM_RATE_LIMIT", "60"))
        self.last_request_time = 0

        # Initialize primary provider
        if self.provider == "anthropic" and ANTHROPIC_AVAILABLE and anthropic_key:
            try:
                self.anthropic_client = anthropic.AsyncAnthropic(
                    api_key=anthropic_key,
                    timeout=30.0,  # 30 second timeout
                    max_retries=3,  # Retry failed requests
                )
                self.logger.info(
                    "âœ… Initialized Anthropic client for production LLM decisions"
                )
                self.llm_available = True
            except Exception as e:
                self.logger.error(f"âŒ Failed to initialize Anthropic client: {e}")
                self.llm_available = False
        elif self.provider == "openai" and OPENAI_AVAILABLE and openai_key:
            try:
                self.openai_client = openai.AsyncOpenAI(
                    api_key=openai_key, timeout=30.0, max_retries=3
                )
                self.logger.info(
                    "âœ… Initialized OpenAI client for production LLM decisions"
                )
                self.llm_available = True
            except Exception as e:
                self.logger.error(f"âŒ Failed to initialize OpenAI client: {e}")
                self.llm_available = False
        else:
            self.logger.warning(
                "âš ï¸ No LLM client available - using intelligent fallback mode"
            )
            self.llm_available = False

        # Initialize fallback enhancement
        if not self.llm_available:
            self._initialize_enhanced_fallback()

    async def create_decision_plan(
        self, context: DecisionContext, decision_type: DecisionType
    ) -> DecisionPlan:
        """
        Production LLM decision plan creation with rate limiting and cost control
        """
        # Check production rate limits and cost controls
        if not await self._check_rate_limits():
            self.logger.warning("âš ï¸ Rate limit exceeded, using enhanced fallback")
            return self._create_enhanced_fallback_plan(context, decision_type)

        if not self._check_cost_limits():
            self.logger.warning("âš ï¸ Daily cost limit exceeded, using enhanced fallback")
            return self._create_enhanced_fallback_plan(context, decision_type)

        if not self._has_llm_client():
            return self._create_enhanced_fallback_plan(context, decision_type)

        try:
            # Build enhanced prompt with production optimizations
            prompt = self._build_enhanced_decision_prompt(context, decision_type)

            # Get LLM response with retry logic
            llm_response = await self._call_llm_with_retry(prompt)

            # Parse and validate the response with better error handling
            decision_plan = self._parse_llm_response_enhanced(
                llm_response, context, decision_type
            )

            # Store for learning and analytics
            self.decision_history.append(decision_plan)
            self._track_llm_usage(decision_plan)

            self.logger.info(
                f"âœ… Created LLM decision plan {decision_plan.plan_id} with {len(decision_plan.actions)} actions"
            )
            return decision_plan

        except Exception as e:
            self.logger.error(f"Error creating decision plan: {str(e)}")
            return self._create_fallback_plan(context, decision_type)

    def _build_decision_prompt(
        self, context: DecisionContext, decision_type: DecisionType
    ) -> str:
        """Build enhanced prompts based on beta testing findings and enterprise requirements"""

        # Extract industry and compliance context
        business_context = context.business_context or ""
        industry_type = self._identify_industry(context)
        compliance_requirements = self._identify_compliance_requirements(context)

        base_prompt = f"""
You are an expert AI security agent specializing in enterprise API security assessment and remediation. Your expertise covers OWASP API Security Top 10 (2023), industry-specific compliance requirements, and modern threat patterns.

ENDPOINT ANALYSIS:
- Endpoint: {context.endpoint_data.get('method', 'UNKNOWN')} {context.endpoint_data.get('path', 'unknown')}
- Industry Context: {industry_type}
- Business Context: {business_context}
- Compliance Requirements: {', '.join(compliance_requirements) if compliance_requirements else 'General security standards'}

SECURITY ASSESSMENT FRAMEWORK:
Focus on OWASP API Security Top 10 (2023):
1. API1:2023 - Broken Object Level Authorization (BOLA/IDOR)
2. API2:2023 - Broken Authentication
3. API3:2023 - Broken Object Property Level Authorization
4. API4:2023 - Unrestricted Resource Consumption
5. API5:2023 - Broken Function Level Authorization
6. API6:2023 - Unrestricted Access to Sensitive Business Flows
7. API7:2023 - Server Side Request Forgery (SSRF)
8. API8:2023 - Security Misconfiguration
9. API9:2023 - Improper Inventory Management
10. API10:2023 - Unsafe Consumption of APIs

INDUSTRY-SPECIFIC CONSIDERATIONS:
{self._get_industry_specific_guidance(industry_type)}

COMPLIANCE REQUIREMENTS:
{self._get_compliance_guidance(compliance_requirements)}

AVAILABLE SECURITY TOOLS:
{self._format_tool_descriptions(context.available_tools)}

RISK ASSESSMENT FRAMEWORK:
- SAFE: Read-only analysis, reporting, no system changes
- LOW: Configuration changes with easy rollback (headers, logging)
- MEDIUM: Authentication/authorization changes requiring approval
- HIGH: Core business logic modifications
- CRITICAL: Data deletion, irreversible changes (NEVER auto-execute)

ENTERPRISE DECISION CRITERIA:
1. Regulatory Compliance: Ensure actions align with {', '.join(compliance_requirements) if compliance_requirements else 'security standards'}
2. Business Continuity: Minimize disruption to operations
3. Risk Mitigation: Address highest-severity vulnerabilities first
4. Audit Trail: Ensure all actions are logged and traceable
5. Rollback Capability: Prefer reversible changes

YOUR TASK: Create a {decision_type.value} that demonstrates enterprise-grade security analysis with specific focus on {industry_type} industry requirements.

RESPONSE FORMAT (JSON):
{{
  "reasoning": "Comprehensive analysis strategy focusing on [specific vulnerabilities expected for this endpoint type]",
  "confidence_score": 0.85,
  "risk_assessment": "[SAFE|LOW|MEDIUM|HIGH]",
  "requires_approval": [true|false],
  "compliance_notes": "Specific compliance considerations for {', '.join(compliance_requirements) if compliance_requirements else 'general standards'}",
  "actions": [
    {{
      "action_id": "action_1",
      "tool_name": "[specific tool from available tools]",
      "parameters": {{"focus": "[OWASP category]", "compliance": "{compliance_requirements[0] if compliance_requirements else 'general'}"}},
      "risk_level": "SAFE",
      "reasoning": "Addresses OWASP API[X]:2023 - [specific vulnerability pattern]",
      "expected_outcome": "Detection of [specific vulnerability types] with compliance validation",
      "estimated_duration": 30,
      "depends_on": null
    }}
  ]
}}
"""

        # Add specific context based on decision type
        if decision_type == DecisionType.ANALYSIS_PLAN:
            base_prompt += f"""
ANALYSIS FOCUS: Create comprehensive security assessment plan
- Start with highest-risk OWASP categories for {industry_type}
- Include compliance validation steps
- Prioritize discovery over remediation
- Plan for thorough documentation and reporting
"""
        elif decision_type == DecisionType.ACTION_SEQUENCE:
            base_prompt += f"""
REMEDIATION FOCUS: Create secure, compliant action sequence
- Address critical vulnerabilities first
- Ensure {', '.join(compliance_requirements) if compliance_requirements else 'security'} compliance
- Plan rollback strategies for each action
- Prioritize safe, automated fixes with user approval for risky changes
"""

        return base_prompt

    def _identify_industry(self, context: DecisionContext) -> str:
        """Identify industry type from context"""
        business_context = (context.business_context or "").lower()
        endpoint_path = str(context.endpoint_data.get("path", "") or "").lower()

        if any(
            term in business_context + endpoint_path
            for term in [
                "payment",
                "fintech",
                "financial",
                "banking",
                "card",
                "transaction",
            ]
        ):
            return "Financial Services"
        elif any(
            term in business_context + endpoint_path
            for term in ["patient", "healthcare", "medical", "health", "hipaa"]
        ):
            return "Healthcare"
        elif any(
            term in business_context + endpoint_path
            for term in ["ecommerce", "commerce", "retail", "shopping", "order"]
        ):
            return "E-commerce"
        elif any(
            term in business_context + endpoint_path
            for term in ["iot", "device", "sensor", "machine"]
        ):
            return "IoT/Manufacturing"
        else:
            return "General Enterprise"

    def _identify_compliance_requirements(self, context: DecisionContext) -> List[str]:
        """Identify compliance requirements from context"""
        business_context = (context.business_context or "").lower()
        endpoint_path = str(context.endpoint_data.get("path", "") or "").lower()
        requirements = []

        if any(
            term in business_context + endpoint_path
            for term in ["payment", "fintech", "financial", "banking", "card"]
        ):
            requirements.extend(["PCI-DSS", "SOX"])
        if any(
            term in business_context + endpoint_path
            for term in ["patient", "healthcare", "medical", "health"]
        ):
            requirements.append("HIPAA")
        if any(
            term in business_context + endpoint_path
            for term in ["eu", "european", "gdpr", "privacy"]
        ):
            requirements.append("GDPR")
        if any(
            term in business_context + endpoint_path
            for term in ["soc2", "enterprise", "audit"]
        ):
            requirements.append("SOC2")

        return list(set(requirements)) or ["OWASP"]

    # Production LLM Enhancement Methods
    async def _check_rate_limits(self) -> bool:
        """Check if we're within rate limits"""
        import time

        current_time = time.time()

        # Reset counter every minute
        if current_time - self.last_request_time > 60:
            self.request_count = 0
            self.last_request_time = current_time

        # Check if under rate limit
        if self.request_count >= self.rate_limit_per_minute:
            return False

        self.request_count += 1
        return True

    def _check_cost_limits(self) -> bool:
        """Check if we're within daily cost limits"""
        return self.daily_cost < self.max_daily_cost

    def _initialize_enhanced_fallback(self):
        """Initialize enhanced fallback capabilities"""
        self.logger.info("ðŸ”§ Initializing enhanced fallback mode with improved accuracy")
        # Enhanced fallback is already implemented in our comprehensive testing

    def _build_enhanced_decision_prompt(
        self, context: DecisionContext, decision_type: DecisionType
    ) -> str:
        """Build production-optimized prompts with better context"""
        base_prompt = self._build_decision_prompt(context, decision_type)

        # Add production enhancements
        enhanced_prompt = f"""
{base_prompt}

PRODUCTION ENHANCEMENTS:
- Prioritize CRITICAL and HIGH severity findings first
- Focus on immediate business impact vulnerabilities
- Consider real-world exploit likelihood based on endpoint exposure
- Provide specific fix recommendations with code examples when possible

CONTEXTUAL INTELLIGENCE:
- Endpoint Traffic Pattern: {self._analyze_endpoint_usage_pattern(context)}
- Security Posture Level: {self._assess_current_security_level(context)}
- Business Criticality: {self._determine_business_criticality(context)}

RESPONSE OPTIMIZATION:
Ensure your response includes specific, actionable recommendations that a security engineer can implement immediately.
"""
        return enhanced_prompt

    async def _call_llm_with_retry(self, prompt: str) -> str:
        """Call LLM with retry logic and circuit breaker"""
        max_retries = 3
        base_delay = 1

        for attempt in range(max_retries):
            try:
                response = await self._call_llm(prompt)
                self._track_successful_request()
                return response
            except Exception as e:
                if attempt == max_retries - 1:
                    raise e

                # Exponential backoff
                delay = base_delay * (2**attempt)
                await asyncio.sleep(delay)
                self.logger.warning(
                    f"LLM request failed (attempt {attempt + 1}), retrying in {delay}s: {e}"
                )

    def _parse_llm_response_enhanced(
        self, response: str, context: DecisionContext, decision_type: DecisionType
    ) -> DecisionPlan:
        """Enhanced LLM response parsing with better error handling"""
        try:
            # Try normal parsing first
            return self._parse_llm_response(response, context, decision_type)
        except Exception as e:
            self.logger.warning(
                f"LLM response parsing failed, using enhanced fallback: {e}"
            )
            # Use enhanced fallback if parsing fails
            return self._create_enhanced_fallback_plan(context, decision_type)

    def _track_llm_usage(self, decision_plan: DecisionPlan):
        """Track LLM usage for analytics and cost management"""
        # Estimate cost based on tokens (approximate)
        estimated_cost = len(str(decision_plan)) * 0.00001  # Rough estimate
        self.daily_cost += estimated_cost

        # Log usage analytics
        self.logger.info(
            f"ðŸ“Š LLM Usage: Plan {decision_plan.plan_id}, Est. Cost: ${estimated_cost:.4f}, Daily Total: ${self.daily_cost:.2f}"
        )

    def _track_successful_request(self):
        """Track successful LLM requests for monitoring"""
        self.logger.debug("âœ… LLM request successful")

    def _analyze_endpoint_usage_pattern(self, context: DecisionContext) -> str:
        """Analyze endpoint usage patterns for better decision making"""
        endpoint_path = context.endpoint_data.get("path", "")
        method = context.endpoint_data.get("method", "")

        # Determine likely usage pattern
        if any(
            term in endpoint_path.lower() for term in ["admin", "internal", "debug"]
        ):
            return "Internal/Admin - Lower traffic, high privilege"
        elif any(
            term in endpoint_path.lower() for term in ["api/v1", "public", "auth"]
        ):
            return "Public API - High traffic, external exposure"
        elif method in ["POST", "PUT", "DELETE"]:
            return "Mutating operations - Medium traffic, high impact"
        else:
            return "Standard operations - Variable traffic"

    def _assess_current_security_level(self, context: DecisionContext) -> str:
        """Assess current security implementation level"""
        security = context.endpoint_data.get("security", [])

        if not security:
            return "Low - No authentication detected"
        elif len(security) == 1:
            return "Medium - Basic authentication present"
        else:
            return "High - Multiple security layers detected"

    def _determine_business_criticality(self, context: DecisionContext) -> str:
        """Determine business criticality of endpoint"""
        path = context.endpoint_data.get("path", "").lower()
        business_context = (context.business_context or "").lower()

        critical_indicators = [
            "payment",
            "transaction",
            "user",
            "account",
            "admin",
            "auth",
        ]
        high_indicators = ["data", "profile", "settings", "config"]

        if any(
            indicator in path + business_context for indicator in critical_indicators
        ):
            return "CRITICAL - Core business functionality"
        elif any(indicator in path + business_context for indicator in high_indicators):
            return "HIGH - Important user functionality"
        else:
            return "MEDIUM - Standard operations"

    def _create_enhanced_fallback_plan(
        self, context: DecisionContext, decision_type: DecisionType
    ) -> DecisionPlan:
        """Create enhanced fallback plan with production optimizations"""
        # This uses our existing comprehensive fallback but with enhanced metadata
        fallback_plan = self._create_fallback_plan(context, decision_type)

        # Enhance with production context
        fallback_plan.reasoning = f"Enhanced fallback analysis for {context.endpoint_data.get('method', 'UNKNOWN')} {context.endpoint_data.get('path', 'unknown')} endpoint with production optimizations"
        fallback_plan.confidence = (
            "85%"  # Higher confidence due to comprehensive testing
        )

        return fallback_plan

    def _get_industry_specific_guidance(self, industry: str) -> str:
        """Get industry-specific security guidance"""
        guidance = {
            "Financial Services": """
- Payment data protection (PCI-DSS compliance)
- Strong authentication and authorization
- Transaction integrity and non-repudiation
- Fraud detection and prevention
- Regulatory reporting and audit trails
- API rate limiting for financial operations
""",
            "Healthcare": """
- Protected Health Information (PHI) security
- Patient data access controls (minimum necessary principle)
- HIPAA compliance for data handling
- Audit logging for all patient data access
- Encryption at rest and in transit
- Business Associate Agreement considerations
""",
            "E-commerce": """
- Customer PII protection
- Payment processing security
- GDPR compliance for EU customers
- Session management and authentication
- Shopping cart and order security
- Customer data breach notification requirements
""",
            "IoT/Manufacturing": """
- Device authentication and authorization
- Command injection prevention
- Firmware security validation
- Network segmentation considerations
- Device lifecycle management
- Industrial control system protection
""",
            "General Enterprise": """
- Standard OWASP API security practices
- Authentication and authorization
- Input validation and output encoding
- Security logging and monitoring
- Configuration management
- Dependency security management
""",
        }
        return guidance.get(industry, guidance["General Enterprise"])

    def _get_compliance_guidance(self, requirements: List[str]) -> str:
        """Get compliance-specific guidance"""
        guidance_map = {
            "PCI-DSS": "Protect cardholder data, implement strong access controls, maintain secure networks",
            "HIPAA": "Protect PHI, implement minimum necessary access, maintain audit logs",
            "GDPR": "Implement privacy by design, ensure data minimization, enable user rights",
            "SOX": "Ensure financial reporting accuracy, implement internal controls",
            "SOC2": "Implement security, availability, processing integrity controls",
            "OWASP": "Follow OWASP API Security Top 10 guidelines",
        }

        return "; ".join(
            [
                guidance_map.get(req, f"Follow {req} requirements")
                for req in requirements
            ]
        )

    def _format_tool_descriptions(self, available_tools: List[str]) -> str:
        """Format tool descriptions for the prompt"""
        descriptions = []
        for tool in available_tools:
            if tool in self.tool_registry:
                descriptions.append(f"- {tool}: {self.tool_registry[tool]}")
        return "\n".join(descriptions)

    async def _call_llm(self, prompt: str) -> str:
        """Call the appropriate LLM with the prompt"""
        if self.anthropic_client:
            response = await self.anthropic_client.messages.create(
                model="claude-3-haiku-20240307",
                max_tokens=2000,
                messages=[{"role": "user", "content": prompt}],
            )
            return response.content[0].text
        elif self.openai_client:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=2000,
                response_format={"type": "json_object"},
            )
            return response.choices[0].message.content
        else:
            raise Exception("No LLM client available")

    def _parse_llm_response(
        self, response: str, context: DecisionContext, decision_type: DecisionType
    ) -> DecisionPlan:
        """Parse LLM response into a structured DecisionPlan"""
        try:
            # Extract JSON from response if needed
            if "```json" in response:
                response = response.split("```json")[1].split("```")[0].strip()

            data = json.loads(response)

            # Create AgentAction objects
            actions = []
            for action_data in data.get("actions", []):
                action = AgentAction(
                    action_id=action_data["action_id"],
                    tool_name=action_data["tool_name"],
                    parameters=action_data["parameters"],
                    risk_level=RiskLevel(action_data["risk_level"].lower()),
                    reasoning=action_data["reasoning"],
                    expected_outcome=action_data["expected_outcome"],
                    estimated_duration=action_data["estimated_duration"],
                    depends_on=action_data.get("depends_on"),
                )
                actions.append(action)

            # Create decision plan
            plan = DecisionPlan(
                plan_id=f"plan_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{context.user_id}",
                decision_type=decision_type,
                actions=actions,
                total_estimated_duration=sum(a.estimated_duration for a in actions),
                confidence_score=data.get("confidence_score", 0.7),
                reasoning=data.get("reasoning", "No reasoning provided"),
                risk_assessment=RiskLevel(
                    data.get("risk_assessment", "medium").lower()
                ),
                requires_approval=data.get("requires_approval", True),
                created_at=datetime.now(),
            )

            return plan

        except Exception as e:
            self.logger.error(f"Error parsing LLM response: {str(e)}")
            return self._create_fallback_plan(context, decision_type)

    def _create_fallback_plan(
        self, context: DecisionContext, decision_type: DecisionType
    ) -> DecisionPlan:
        """Create an intelligent fallback plan when LLM is unavailable"""
        endpoint = context.endpoint_data
        business_context = context.business_context or ""

        # Create smarter fallback actions based on endpoint analysis
        actions = []

        # Always start with basic security scan
        actions.append(
            AgentAction(
                action_id="fallback_security_scan",
                tool_name="security_vulnerability_scan",
                parameters={
                    "target": "comprehensive"
                    if "payment" in str(endpoint.get("path", "") or "").lower()
                    else "general",
                    "depth": "comprehensive",
                },
                risk_level=RiskLevel.SAFE,
                reasoning="Comprehensive security vulnerability scan",
                expected_outcome="Detailed security assessment with vulnerability identification",
                estimated_duration=45,
            )
        )

        # Add authentication analysis if endpoint has auth requirements
        if endpoint.get("security"):
            actions.append(
                AgentAction(
                    action_id="fallback_auth_analysis",
                    tool_name="auth_mechanism_analysis",
                    parameters={"focus": "authentication_strength"},
                    risk_level=RiskLevel.SAFE,
                    reasoning="Analyze authentication mechanisms and identify weaknesses",
                    expected_outcome="Authentication security assessment",
                    estimated_duration=30,
                )
            )
        else:
            # No authentication - critical issue
            actions.append(
                AgentAction(
                    action_id="fallback_missing_auth",
                    tool_name="auth_mechanism_analysis",
                    parameters={"focus": "missing_authentication"},
                    risk_level=RiskLevel.SAFE,
                    reasoning="Identify missing authentication requirements",
                    expected_outcome="Authentication gap analysis",
                    estimated_duration=20,
                )
            )

        # Add data exposure check for sensitive endpoints
        sensitive_paths = [
            "payment",
            "user",
            "patient",
            "account",
            "profile",
            "personal",
        ]
        if any(
            sensitive in str(endpoint.get("path", "") or "").lower()
            for sensitive in sensitive_paths
        ):
            actions.append(
                AgentAction(
                    action_id="fallback_data_exposure",
                    tool_name="data_exposure_check",
                    parameters={"sensitivity_level": "high"},
                    risk_level=RiskLevel.SAFE,
                    reasoning="Check for sensitive data exposure in responses",
                    expected_outcome="Data exposure risk assessment",
                    estimated_duration=25,
                )
            )

        # Add compliance check based on business context
        compliance_standards = []
        if (
            "healthcare" in business_context.lower()
            or "patient" in str(endpoint.get("path", "") or "").lower()
        ):
            compliance_standards.extend(["HIPAA", "GDPR"])
        if (
            "payment" in business_context.lower()
            or "fintech" in business_context.lower()
        ):
            compliance_standards.extend(["PCI-DSS", "GDPR"])
        if "banking" in business_context.lower():
            compliance_standards.extend(["SOX", "PCI-DSS", "GDPR"])

        if compliance_standards:
            actions.append(
                AgentAction(
                    action_id="fallback_compliance",
                    tool_name="compliance_check",
                    parameters={"standards": compliance_standards},
                    risk_level=RiskLevel.SAFE,
                    reasoning=f"Check compliance with {', '.join(compliance_standards)} standards",
                    expected_outcome="Compliance assessment report",
                    estimated_duration=40,
                )
            )

        # Determine risk level based on context
        risk_level = RiskLevel.SAFE
        if (
            "critical" in business_context.lower()
            or "healthcare" in business_context.lower()
            or "banking" in business_context.lower()
        ):
            risk_level = RiskLevel.MEDIUM

        total_duration = sum(action.estimated_duration for action in actions)

        return DecisionPlan(
            plan_id=f"enhanced_fallback_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            decision_type=decision_type,
            actions=actions,
            total_estimated_duration=total_duration,
            confidence_score=0.7,  # Higher confidence with smarter fallback
            reasoning=f"Enhanced fallback analysis for {endpoint.get('method', 'UNKNOWN')} {endpoint.get('path', 'unknown')} endpoint with {len(actions)} security checks",
            risk_assessment=risk_level,
            requires_approval=risk_level != RiskLevel.SAFE,
            created_at=datetime.now(),
        )

    def _has_llm_client(self) -> bool:
        """Check if any LLM client is available"""
        return self.anthropic_client is not None or self.openai_client is not None

    async def execute_action(
        self, action: AgentAction, context: DecisionContext
    ) -> Dict[str, Any]:
        """
        Execute a single action with real autonomous capabilities
        """
        self.logger.info(
            f"ðŸ¤– Autonomously executing action {action.action_id}: {action.tool_name}"
        )

        try:
            # Import autonomous security tools
            import sys
            import os

            # Add the backend/src directory to path if not already there
            backend_src_path = os.path.join(os.path.dirname(__file__), ".")
            if backend_src_path not in sys.path:
                sys.path.insert(0, backend_src_path)

            from autonomous_security_tools import SecurityToolExecutor

            # Create tool executor
            executor = SecurityToolExecutor()

            # Execute based on tool type
            if action.tool_name == "security_vulnerability_scan":
                result = await executor.execute_security_vulnerability_scan(
                    action.parameters, context.endpoint_data
                )
            elif action.tool_name == "auth_mechanism_analysis":
                result = await executor.execute_auth_mechanism_analysis(
                    action.parameters, context.endpoint_data
                )
            elif action.tool_name == "compliance_check":
                result = await executor.execute_compliance_check(
                    action.parameters, context.endpoint_data, context.business_context
                )
            elif action.tool_name == "auto_fix_security_headers":
                result = await executor.execute_auto_fix_security_headers(
                    action.parameters, context.endpoint_data
                )
            elif action.tool_name == "advanced_remediation":
                result = await executor.execute_advanced_remediation(
                    action.parameters, context.endpoint_data, context.business_context
                )
            elif action.tool_name == "smart_code_refactoring":
                result = await executor.execute_smart_code_refactoring(
                    action.parameters, context.endpoint_data, context.business_context
                )
            elif action.tool_name == "devops_security_scan":
                result = await executor.execute_devops_security_scan(
                    action.parameters, context.endpoint_data, context.business_context
                )
            elif action.tool_name == "database_security_audit":
                result = await executor.execute_database_security_audit(
                    action.parameters, context.endpoint_data, context.business_context
                )
            else:
                # Fallback for unknown tools
                result = {
                    "action_id": action.action_id,
                    "status": "unsupported",
                    "result": f"Tool {action.tool_name} not implemented",
                    "timestamp": datetime.now().isoformat(),
                }

            self.logger.info(f"âœ… Action executed successfully: {result.get('status')}")
            return result

        except ImportError as e:
            self.logger.error(f"âŒ Failed to import autonomous tools: {str(e)}")
            return {
                "action_id": action.action_id,
                "status": "failed",
                "error": f"Module import failed: {str(e)}",
                "result": f"Failed to import autonomous tools for {action.tool_name}",
                "timestamp": datetime.now().isoformat(),
            }
        except Exception as e:
            self.logger.error(f"âŒ Action execution failed: {str(e)}")
            return {
                "action_id": action.action_id,
                "status": "failed",
                "error": str(e),
                "result": f"Failed to execute {action.tool_name}: {str(e)}",
                "timestamp": datetime.now().isoformat(),
            }

    def learn_from_feedback(self, plan_id: str, user_feedback: Dict[str, Any]):
        """
        Learn from user feedback to improve future decisions
        """
        # Find the plan
        plan = next((p for p in self.decision_history if p.plan_id == plan_id), None)
        if not plan:
            return

        # Store feedback for future improvement
        # This could be used to fine-tune prompts or decision logic
        self.logger.info(f"Learning from feedback for plan {plan_id}: {user_feedback}")

    def get_decision_summary(self, plan: DecisionPlan) -> Dict[str, Any]:
        """
        Get a user-friendly summary of a decision plan
        """
        return {
            "plan_id": plan.plan_id,
            "summary": plan.reasoning,
            "total_actions": len(plan.actions),
            "estimated_duration_minutes": plan.total_estimated_duration // 60,
            "confidence": f"{plan.confidence_score:.0%}",
            "risk_level": plan.risk_assessment.value,
            "requires_approval": plan.requires_approval,
            "actions_preview": [
                {
                    "action": action.tool_name,
                    "purpose": action.reasoning,
                    "risk": action.risk_level.value,
                }
                for action in plan.actions[:3]  # Show first 3 actions
            ],
        }
